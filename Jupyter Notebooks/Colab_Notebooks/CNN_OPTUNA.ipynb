{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpSFh2BjjTYq"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "\n",
        "import tensorflow as tf\n",
        "import optuna\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "import math\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GOLBAL VARIABLES NEEDED TO RUN THE EXPERIMENT\n",
        "\n",
        "# 1] ENTER DATA DIRECTORIES\n",
        "base_dir  = \"/content/drive/MyDrive/CIFAR_Dataset/CIFAR_10/Batch3_1+10K\"\n",
        "train_dir = base_dir+'/cifar10_train.npz'\n",
        "val_dir   = base_dir+'/cifar10_val.npz'\n",
        "test_dir  = base_dir+'/cifar10_test.npz'\n",
        "# 2] ENTER DIRECTORY TO SAVE OPTUNA's TRAINED MODEL\n",
        "OPTUNA_MODEL_DIRECTORY = \"/content/drive/MyDrive/CIFAR_Dataset/CIFAR_10/cnn_optuna.pickle\"\n",
        "\n",
        "NUMBER_OF_DENSE_LAYERS = 3 # or 5; Includes the output layer.\n",
        "TYPE_OF_SAMPLER = 'grid' #  'grid', 'random', 'qmc', 'tpe' \n",
        "NUM_OPTUNA_TRIALS = 10"
      ],
      "metadata": {
        "id": "BVb9XYleuXxe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**OPTUNA STUDY**"
      ],
      "metadata": {
        "id": "75GpXu5Zyv9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Loading Data**"
      ],
      "metadata": {
        "id": "p3xYgWyVuZVt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qOpQqPRDl1Qh"
      },
      "outputs": [],
      "source": [
        "# Enter training, validation and testing dataset directories for CIFAR-10 datasets.\n",
        "def preprocess_image_input(input_images): # Only used when resent50 is selected as model_type below\n",
        "  input_images = input_images.astype('float32')\n",
        "  output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
        "  return output_ims\n",
        "\n",
        "def data_loader( train_dir, val_dir, test_dir, model_type ): # model_type : 'cnn' OR 'resnet50' ( Please specify only one of these in sttring format )\n",
        "\n",
        "    train_dataset = np.load(train_dir)\n",
        "    val_dataset   = np.load(val_dir)\n",
        "    test_dataset  = np.load(test_dir)\n",
        "\n",
        "    y_train = train_dataset['y_train'].astype(\"float32\")\n",
        "    y_val   = val_dataset['y_val'].astype(\"float32\") \n",
        "    y_test  = test_dataset['y_test'].astype(\"float32\") \n",
        "\n",
        "    x_train = train_dataset['x_train'].astype(\"float32\")\n",
        "    x_val   = val_dataset['x_val'].astype(\"float32\")\n",
        "    x_test  = test_dataset['x_test'].astype(\"float32\") \n",
        "\n",
        "    if model_type == 'cnn':\n",
        "        x_train, x_val, x_test = x_train/255, x_val/255, x_test/255\n",
        "    elif model_type == 'resnet50':\n",
        "        x_train = preprocess_image_input(x_train)\n",
        "        x_val = preprocess_image_input(x_val)\n",
        "        x_test = preprocess_image_input(x_test)\n",
        "    elif model_type not in ['cnn', 'resnet50']:\n",
        "        raise ValueError('Error: Please enter correct \\'model_type\\' variable value. Correct values are \\'cnn\\' or \\'resnet50\\' (strings).')\n",
        "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
        "\n",
        "## ============================================  LOADING DATA  =====================================================\n",
        "x_train, x_val, x_test, y_train, y_val, y_test = data_loader( train_dir, val_dir, test_dir, model_type='cnn' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvjtltoIrR_J"
      },
      "source": [
        "##**Custom Model Defining**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wE3rshsvKcuO"
      },
      "outputs": [],
      "source": [
        "class CNN:\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        self.input_size = input_size\n",
        "        self.num_classes = num_classes\n",
        "    \n",
        "    def generate_model(self, layer_info = None ):\n",
        "        model = tf.keras.Sequential()\n",
        "\n",
        "        model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(self.input_size, self.input_size, 3)))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "        model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "        model.add(tf.keras.layers.GlobalAveragePooling2D()) # add GlobalAveragePooling2D layer\n",
        "        # model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "        if layer_info is not None:\n",
        "            for layer in layer_info[:-1]:\n",
        "                layer_params = layer['params']\n",
        "                if layer['type'] == 'dense':\n",
        "                    model.add(tf.keras.layers.Dense(**layer_params))\n",
        "                    model.add(tf.keras.layers.BatchNormalization())\n",
        "            for layer in layer_info[-1:]:\n",
        "                layer_params = layer['params']\n",
        "                if layer['type'] == 'dense':\n",
        "                    model.add(tf.keras.layers.Dense(**layer_params))\n",
        "\n",
        "        # Calculate the number of trainable parameters in the model\n",
        "        trainable_count = sum(tf.keras.backend.count_params(weights) for weights in model.trainable_weights)\n",
        "        print(f\"Trainable parameters: {trainable_count:,}\")\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "R3-RzK33q5sy"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def loss_function_optuna( y_dataset, logits, loss ): # logits = model(x_dataset)\n",
        "    total_loss = loss(y_dataset, logits)\n",
        "    total_loss = tf.cast( total_loss, dtype=tf.float32 )\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wAlCfWNqrZC9"
      },
      "outputs": [],
      "source": [
        "def fmin_loss( model, loss_function, optimizer, batch_size , epochs, record = True ):      # lamda, not exp(lamda), Works with both tf.Variable and tf.constant type lambda input, (or just scalar)\n",
        "    # total_loss0 = 1e20\n",
        "    train_df = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
        "    train_df = train_df.shuffle(buffer_size = 1024).batch(batch_size)\n",
        "\n",
        "    All_Epoch_Gradients, All_Epoch_Weights = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        weights0 = [var.numpy() for var in model.trainable_weights] # Getting only trainable weights at which the gradient is being calculated.\n",
        "        # Note : model.get_weights() retrieves all the weights (including non-trainable)\n",
        "\n",
        "        Step_Gradient, Num_batch = [], 0\n",
        "        \n",
        "        for step,(x_train_,y_train_) in enumerate(train_df):\n",
        "            # print(\"Step == \", step)\n",
        "            with tf.GradientTape(persistent = True) as tape:\n",
        "\n",
        "                logits = model(x_train_, training=True)\n",
        "                total_loss1 = loss_function_optuna( y_train_, logits, loss = loss_function ) \n",
        "\n",
        "            vars_list = model.trainable_weights\n",
        "            grads = tape.gradient(total_loss1, vars_list)      # for ref  - https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough \n",
        "            optimizer.apply_gradients(zip(grads,vars_list))\n",
        "\n",
        "            if record : \n",
        "                if step == 0 : \n",
        "                    Step_Gradient = grads\n",
        "                    # print( Step_Gradient )\n",
        "                else:\n",
        "                    for idx in range(len(Step_Gradient)):\n",
        "                        Step_Gradient[idx] =  tf.add(Step_Gradient[idx], grads[idx])\n",
        "            Num_batch = step\n",
        "        \n",
        "    # total_loss0   = total_loss1\n",
        "        if record : \n",
        "            Step_Gradient = [ i/Num_batch for i in Step_Gradient ] \n",
        "            All_Epoch_Gradients.append( Step_Gradient )\n",
        "            All_Epoch_Weights.append(weights0)\n",
        "            \n",
        "    if record :\n",
        "        return All_Epoch_Gradients, All_Epoch_Weights\n",
        "    else: \n",
        "        return 0, 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riEC0fSDGxRP"
      },
      "source": [
        "####Note: Training and Local Tuning is done based on Loss and not the Accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3TADO0MDrjwS"
      },
      "outputs": [],
      "source": [
        "def optuna_optimizer(trial):\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    alphas = [ trial.suggest_float(f'regularization{i}', 1e-6, 1e-1, log=True) for i in range(NUMBER_OF_DENSE_LAYERS) ]\n",
        "\n",
        "    # Define the new model\n",
        "    input_shape = 32\n",
        "    output_shape = 10\n",
        "\n",
        "    layer_info_ = [ {'type': 'dense', 'params': {'units': 64, 'activation': 'relu', 'kernel_regularizer':tf.keras.regularizers.l2(i)}} for i in alphas[:-1]]\n",
        "    layer_info_ += [ {'type': 'dense', 'params': {'units': output_shape, 'activation': 'softmax', 'kernel_regularizer':tf.keras.regularizers.l2(alphas[-1])}} ]\n",
        "\n",
        "    model = CNN( input_shape, output_shape).generate_model( layer_info_ )\n",
        "    \n",
        "    # Optimizing\n",
        "    optimizer         = tf.keras.optimizers.Adam()\n",
        "    loss_function     = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    All_Epoch_Gradients, All_Epoch_Weights = fmin_loss( model, loss_function, optimizer,  128,  100, True)\n",
        "\n",
        "    # Getting Scores\n",
        "    cce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    sca = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    y_pred_train = model.predict(x_train)\n",
        "    train_loss_unregularized = cce(y_train, y_pred_train).numpy()\n",
        "    train_acc = sca(y_train, y_pred_train).numpy()\n",
        "\n",
        "    y_pred_val = model.predict(x_val)\n",
        "    val_loss_unregularized = cce(y_val, y_pred_val).numpy()\n",
        "    val_acc = sca(y_val, y_pred_val).numpy()\n",
        "\n",
        "    y_pred_test = model.predict(x_test)\n",
        "    test_loss_unregularized = cce(y_test, y_pred_test).numpy()\n",
        "    test_acc = sca(y_test, y_pred_test).numpy()\n",
        "\n",
        "    print(\"\\nTraining:  Loss \", train_loss_unregularized, \"  Accuracy\", train_acc*100 )\n",
        "    print(\"Validation: Loss \", val_loss_unregularized, \"  Accuracy\", val_acc*100 )\n",
        "    print(\"Test:       Loss\", test_loss_unregularized, \"  Accuracy\", test_acc*100, \"\\n\\n\")\n",
        "\n",
        "    with open(\"{}.pickle\".format(trial.number), \"wb\") as fout:\n",
        "        pkl.dump(model, fout)\n",
        "\n",
        "    with open(\"training_info_{}.pickle\".format(trial.number), \"wb\") as fout:\n",
        "        Dict_ = { \"Gradients\" : All_Epoch_Gradients, \"Weights\" : All_Epoch_Weights }\n",
        "        pkl.dump(Dict_, fout)\n",
        "    \n",
        "    score = val_loss_unregularized\n",
        "    return(score)\n",
        "\n",
        "\n",
        "def optuna_training( num_trials ):\n",
        "\n",
        "    time1 = datetime.now()\n",
        "\n",
        "    if TYPE_OF_SAMPLER == 'grid': \n",
        "        search_space = { f\"regularization{i}\" : list(np.linspace(1e-6,1e-1,10)) for i in range(NUMBER_OF_DENSE_LAYERS) }\n",
        "        study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space), direction = \"minimize\")\n",
        "    elif TYPE_OF_SAMPLER == 'random': \n",
        "        study = optuna.create_study(sampler=optuna.samplers.RandomSampler(), direction = \"minimize\")\n",
        "    elif TYPE_OF_SAMPLER == 'qmc': \n",
        "        study = optuna.create_study(sampler=optuna.samplers.QMCSampler(), direction = \"minimize\")\n",
        "    elif TYPE_OF_SAMPLER == 'tpe': \n",
        "        study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction = \"minimize\")\n",
        "    else:\n",
        "        print(\"...Mention the correct sampler name...\")\n",
        "\n",
        "    study.optimize(optuna_optimizer, n_trials = num_trials)\n",
        "\n",
        "    print('\\n\\n')\n",
        "    trial = study.best_trial\n",
        "    print(\"Best Score: \", trial.value)\n",
        "    print(\"Best Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"  {}: {}\".format(key, value))\n",
        "\n",
        "    print( \"\\n\\n \", \"Trial Number: \", trial.number, \"\\n\" )\n",
        "\n",
        "    time2 = datetime.now()\n",
        "    delta = time2 - time1\n",
        "    print(f\"Time difference is {delta.total_seconds()} seconds\")\n",
        "\n",
        "    # Loading Best OPTUNA model to get initial feasible weights for trainable layers\n",
        "    with open(\"{}.pickle\".format(trial.number), \"rb\") as fin:\n",
        "        best_clf = pkl.load(fin)\n",
        "\n",
        "    with open(\"training_info_{}.pickle\".format(trial.number), \"rb\") as fin_:\n",
        "        wt_grad = pkl.load(fin_)\n",
        "\n",
        "    # Getting Optimal Model's Weights and Gradients values for Hessian Approximations\n",
        "    weight_sets = wt_grad['Weights']\n",
        "    grad_sets   = wt_grad['Gradients']\n",
        "\n",
        "    # Getting Optimal Model's Weights and Hyperparameters\n",
        "    full_weights_list          = best_clf.get_weights()\n",
        "    trainable_weights_list     = best_clf.trainable_weights\n",
        "    init_hyperparameters = [ tf.Variable(value) for key, value in trial.params.items() ]\n",
        "\n",
        "    return trainable_weights_list, full_weights_list, init_hyperparameters, weight_sets, grad_sets, delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfHTK01RyTWH"
      },
      "outputs": [],
      "source": [
        "trainable_weights_list, full_weights_list, init_hyperparameters, weight_sets, grad_sets, optuna_time = optuna_training(NUM_OPTUNA_TRIALS)\n",
        "\n",
        "model__ = [ trainable_weights_list, full_weights_list, init_hyperparameters, weight_sets, grad_sets, optuna_time ]\n",
        "# LOADING OPTUNA TRAINED MODELS\n",
        "with open(OPTUNA_MODEL_DIRECTORY, \"wb\") as fout:\n",
        "        pkl.dump(model__, fout)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vh9VgN-Ay_Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o79OJUrc0m13"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "CQEJOPWprMm9",
        "HvjtltoIrR_J",
        "IOtTTaR7BAt-",
        "-4lQQRm4aWSK",
        "RlBwIDtt6JsO"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}