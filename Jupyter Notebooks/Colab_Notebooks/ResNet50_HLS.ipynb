{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gurobipy\n",
        "\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "from datetime import datetime\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBMUFRrIbe1m",
        "outputId": "1eb0a052-5a1c-4225-ff41-43d385dc2d54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gurobipy\n",
            "  Downloading gurobipy-10.0.1-cp310-cp310-manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gurobipy\n",
            "Successfully installed gurobipy-10.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GOLBAL VARIABLES NEEDED TO RUN THE EXPERIMENT\n",
        "\n",
        "# 1] ENTER DATA DIRECTORIES\n",
        "base_dir  = \"/content/drive/MyDrive/CIFAR_Dataset/CIFAR_10/Batch3_1+10K\"\n",
        "train_dir = base_dir+'/cifar10_train.npz'\n",
        "val_dir   = base_dir+'/cifar10_val.npz'\n",
        "test_dir  = base_dir+'/cifar10_test.npz'\n",
        "# 2] ENTER DIRECTORY OF OPTUNA's TRAINED MODEL\n",
        "OPTUNA_MODEL_DIRECTORY = \"/content/drive/MyDrive/CIFAR_Dataset/CIFAR_10/resnet_optuna.pickle\"\n",
        "\n",
        "#3] GUROBI ENVIRONMENT WITH ACADEMIC LICENSE DETAILS\n",
        "# In case you do not have a web license set up and find it difficult, Please see readmi file for further details.\n",
        "ENV = gp.Env( empty=True )\n",
        "ENV.setParam( 'WLSACCESSID', 'xxxxxxxxxxx' )   \n",
        "ENV.setParam( 'WLSSECRET', 'xxxxxxxxx' )    \n",
        "ENV.setParam( 'LICENSEID', xxxx )\n",
        "ENV.setParam( 'OutputFlag', 0 )      # To Turn-off Logs\n",
        "ENV.start()\n",
        "\n",
        "#4] RESULTS OUTPUT DIRECTORY\n",
        "full_output_directory   = \"/content/drive/MyDrive/CIFAR_Dataset/CIFAR_10/resnet_result.xlsx\"  # Second part is the name of the file. USE .xlsx in the end to save the result as excel file."
      ],
      "metadata": {
        "id": "ERsBpMb3MwSP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hyper Local Search**"
      ],
      "metadata": {
        "id": "vz2G9KEF4VZy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQEJOPWprMm9"
      },
      "source": [
        "##**Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter training, validation and testing dataset directories for CIFAR-10 datasets.\n",
        "def preprocess_image_input(input_images): # Only used when resent50 is selected as model_type below\n",
        "  input_images = input_images.astype('float32')\n",
        "  output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
        "  return output_ims\n",
        "\n",
        "def data_loader( train_dir, val_dir, test_dir, model_type ): # model_type : 'cnn' OR 'resnet50' ( Please specify only one of these in sttring format )\n",
        "\n",
        "    train_dataset = np.load(train_dir)\n",
        "    val_dataset   = np.load(val_dir)\n",
        "    test_dataset  = np.load(test_dir)\n",
        "\n",
        "    y_train = train_dataset['y_train'].astype(\"float32\")\n",
        "    y_val   = val_dataset['y_val'].astype(\"float32\") \n",
        "    y_test  = test_dataset['y_test'].astype(\"float32\") \n",
        "\n",
        "    x_train = train_dataset['x_train'].astype(\"float32\")\n",
        "    x_val   = val_dataset['x_val'].astype(\"float32\")\n",
        "    x_test  = test_dataset['x_test'].astype(\"float32\") \n",
        "\n",
        "    if model_type == 'cnn':\n",
        "        x_train, x_val, x_test = x_train/255, x_val/255, x_test/255\n",
        "    elif model_type == 'resnet50':\n",
        "        x_train = preprocess_image_input(x_train)\n",
        "        x_val = preprocess_image_input(x_val)\n",
        "        x_test = preprocess_image_input(x_test)\n",
        "    elif model_type not in ['cnn', 'resnet50']:\n",
        "        raise ValueError('Error: Please enter correct \\'model_type\\' variable value. Correct values are \\'cnn\\' or \\'resnet50\\' (strings).')\n",
        "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
        "\n",
        "## ============================================  LOADING DATA  =====================================================\n",
        "x_train, x_val, x_test, y_train, y_val, y_test = data_loader( train_dir, val_dir, test_dir, model_type='resnet50' )\n",
        "\n",
        "input_shape  = x_train.shape[-3:]\n",
        "output_shape = 10"
      ],
      "metadata": {
        "id": "1AX_L6k0e1N3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOei8iqAr6tD"
      },
      "source": [
        "####**ResNet50 Model** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D0zIgH-ITtet"
      },
      "outputs": [],
      "source": [
        "class CNN:\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "    def generate_model(self, layer_info = None ):\n",
        "        UpSampling = 224/self.input_shape[0]\n",
        "\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Input(shape=self.input_shape))\n",
        "        model.add( tf.keras.layers.UpSampling2D(size=(UpSampling,UpSampling)))\n",
        "        model.add(tf.keras.applications.resnet50.ResNet50(include_top = False, weights = 'imagenet', input_shape = (224, 224, 3),  pooling = 'avg'))\n",
        "        if layer_info is not None:\n",
        "            for layer in layer_info[:-1]:\n",
        "                layer_params = layer['params']\n",
        "                if layer['type'] == 'dense':\n",
        "                    model.add(tf.keras.layers.Dense(**layer_params))\n",
        "                    model.add(tf.keras.layers.BatchNormalization())\n",
        "            for layer in layer_info[-1:]:\n",
        "                layer_params = layer['params']\n",
        "                if layer['type'] == 'dense':\n",
        "                    model.add(tf.keras.layers.Dense(**layer_params))\n",
        "        model.layers[1].trainable = False\n",
        "        # Calculate the number of trainable parameters in the model\n",
        "        trainable_count = sum(tf.keras.backend.count_params(weights) for weights in model.trainable_weights)\n",
        "        print(f\"Trainable parameters: {trainable_count:,}\")\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_information( output_classes, dense_units, dense_kernel_regularizers=None, output_kernel_regularizer=None, include_flatten=True ):\n",
        "    layers = []\n",
        "    if include_flatten: layers.append({'type': 'flatten', 'params': {}})\n",
        "    if dense_kernel_regularizers != None:\n",
        "        for units,lamda in zip(dense_units,dense_kernel_regularizers):\n",
        "            layers.append( {'type': 'dense', 'params': {'units': units, 'activation': 'relu', 'kernel_regularizer':tf.keras.regularizers.l2(lamda)}} )\n",
        "    else:\n",
        "        for units in dense_units:\n",
        "            layers.append( {'type': 'dense', 'params': {'units': units, 'activation': 'relu'}} )\n",
        "    if output_kernel_regularizer == None:\n",
        "        layers.append( {'type': 'dense', 'params': {'units': output_classes, 'activation': 'softmax'}} )\n",
        "    else: layers.append( {'type': 'dense', 'params': {'units': output_classes, 'activation': 'softmax', 'kernel_regularizer':tf.keras.regularizers.l2(output_kernel_regularizer)}} )\n",
        "\n",
        "    return layers"
      ],
      "metadata": {
        "id": "dyh2ZS7mXBML"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Loading OPTUNA trained models**"
      ],
      "metadata": {
        "id": "UgVEdWXukEHa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fM7X46-WBxbM"
      },
      "outputs": [],
      "source": [
        "# LOADING OPTUNA TRAINED MODELS\n",
        "with open(OPTUNA_MODEL_DIRECTORY, \"rb\") as fout:\n",
        "        list__ = pkl.load(fout)\n",
        "[ trainable_weights_list, full_weights_list, init_hyperparameters, weight_sets, grad_sets, optuna_time ] = list__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOtTTaR7BAt-"
      },
      "source": [
        "##**Getting Gradients and Hessian**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0kLl2xc4dHi",
        "outputId": "f7950bf4-e71f-4fb5-da6a-a43c879c5b9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 5s 0us/step\n",
            "Trainable parameters: 32,986\n"
          ]
        }
      ],
      "source": [
        "# WARNING : \"model_\" variable is used globally, so do not move the cell without editing the code.\n",
        "tf.keras.backend.clear_session()\n",
        "hidden_dense_layers = [16] # Defining new dense layers in the end.\n",
        "# model_= NeuralNetwork( input_shape, layer_info_).generate_model()\n",
        "layer_info_ = layer_information( output_shape, hidden_dense_layers, include_flatten=False )\n",
        "model_= CNN( input_shape, output_shape).generate_model( layer_info_ )\n",
        "model_.build(input_shape) # Unless .build is called, gradient tape watch list will be empty\n",
        "model_.set_weights( full_weights_list )\n",
        "\n",
        "# GETTING INDEXES OF TRAINABLE WEIGHTS ONLY\n",
        "all_weights = model_.get_weights()\n",
        "trainable_weights = model_.trainable_weights\n",
        "index_set = []\n",
        "for idx in range(len(all_weights)):\n",
        "    weight = all_weights[idx]\n",
        "    var_name = model_.weights[idx].name.split(':')[0]\n",
        "    if var_name in [t.name.split(':')[0] for t in trainable_weights]:\n",
        "        index_set.append(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C5WYD5j86J-K"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "# Layer_Weights_ToRegularize : Only give list of weights to be used on regularization term. (Do not include bias weights)\n",
        "def loss_function( y_dataset, logits, Layer_Weights_ToRegularize = None,\n",
        "                  Regularization_Parameters = None, loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ): # logits = model(x_dataset)\n",
        "\n",
        "    total_loss = loss(y_dataset, logits)\n",
        "    total_loss = tf.cast( total_loss, dtype=tf.float32 )\n",
        "\n",
        "    if Regularization_Parameters == None or Layer_Weights_ToRegularize == None:  return total_loss\n",
        "    for weight, lamda in zip( Layer_Weights_ToRegularize, Regularization_Parameters ): \n",
        "\n",
        "        if not tf.is_tensor(weight): tf.convert_to_tensor(weight)\n",
        "\n",
        "        regularization  = lamda * tf.reduce_sum(tf.square(weight))   # Element-wise square -> Adding all terms -> Multiply by lamda\n",
        "        regularization /= tf.cast(tf.size(weight), dtype=tf.float32) # Get number of parameters (N)\n",
        "        total_loss     += regularization\n",
        "        \n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QSg_4PVFtoNS"
      },
      "outputs": [],
      "source": [
        "# Extra functions required for operations during hessian approximation\n",
        "def compute_outer_product(tensors1, tensors2): # USES CPU RAM\n",
        "\n",
        "    arr1, arr2 = tensors1[0].reshape(-1), tensors2[0].reshape(-1)\n",
        "    for tensor1, tensor2 in zip( tensors1[1:], tensors2[1:] ):\n",
        "        arr1 = np.concatenate((arr1, tensor1), axis=None)\n",
        "        arr2 = np.concatenate((arr2, tensor2), axis=None)\n",
        "    return np.outer(arr1, arr2)\n",
        "\n",
        "def compute_inner_product( tensors1, tensors2 ):\n",
        "\n",
        "    flattened_vector1 = [tf.reshape(t, [-1]) for t in tensors1]\n",
        "    flattened_vector1 = tf.concat(flattened_vector1, axis=-1)\n",
        "\n",
        "    flattened_vector2 = [tf.reshape(t, [-1]) for t in tensors2]\n",
        "    flattened_vector2 = tf.concat(flattened_vector2, axis=-1)\n",
        "\n",
        "    flattened_shape = flattened_vector1.shape.as_list()\n",
        "\n",
        "    matrix1 = tf.linalg.LinearOperatorFullMatrix(\n",
        "        tf.reshape(flattened_vector1, [flattened_shape[0], 1])\n",
        "    )\n",
        "    matrix2 = tf.linalg.LinearOperatorFullMatrix(\n",
        "        tf.reshape(flattened_vector2, [1,flattened_shape[0]])\n",
        "    )\n",
        "    return tf.linalg.matmul(matrix2, matrix1).to_dense()\n",
        "\n",
        "# COMPUTING VALIDATION LOSS COEFFICIENTS\n",
        "def gradient_validation( batch_size = 32 ):\n",
        "\n",
        "    val_df = tf.data.Dataset.from_tensor_slices((x_val,y_val))\n",
        "    val_df = val_df.shuffle(buffer_size = 1024).batch(batch_size)\n",
        "\n",
        "    Step_Gradient, Num_batch = [], 0\n",
        "    for step,(x_t,y_t) in enumerate(val_df):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "            logits = model_(x_t)\n",
        "            loss_  = loss_function( y_t, logits )\n",
        "\n",
        "        vars_list = model_.trainable_weights\n",
        "        grads = tape.gradient(loss_, vars_list)\n",
        "\n",
        "        if step == 0 : Step_Gradient = grads\n",
        "        else:\n",
        "            for idx in range(len(Step_Gradient)):\n",
        "                Step_Gradient[idx] =  tf.add(Step_Gradient[idx], grads[idx])\n",
        "        Num_batch = step\n",
        "\n",
        "    Step_Gradient = [ i/Num_batch for i in Step_Gradient ] \n",
        "    return Step_Gradient\n",
        "\n",
        "# SYMMETRIC RANK-1 HESSIAN APPROXIMATION \n",
        "def sr1_hessian_approximation( weight_sets, grad_sets  ):\n",
        "\n",
        "    trainable_count = sum(tf.keras.backend.count_params(weights) for weights in model_.trainable_weights)\n",
        "    B_k             = np.identity(trainable_count, dtype = 'float32')\n",
        "\n",
        "    weight_sets, grad_sets = weight_sets[:50], grad_sets[:50]\n",
        "\n",
        "    total_iterations = len(weight_sets) - 1\n",
        "    total_weights    = len(grad_sets[0])\n",
        "\n",
        "    for iter in range( 1, total_iterations + 1  ):\n",
        "\n",
        "        y_k, s_k = [], []\n",
        "        for idx in range( total_weights ):\n",
        "\n",
        "            y_k.append( (grad_sets[iter][idx] - grad_sets[iter-1][idx]).numpy() )\n",
        "            s_k.append( weight_sets[iter][idx] - weight_sets[iter-1][idx] )\n",
        "\n",
        "        arr1, arr2 = y_k[0].reshape(-1), s_k[0].reshape(-1)\n",
        "        for tensor1, tensor2 in zip( y_k[1:], s_k[1:] ):\n",
        "            arr1 = np.concatenate((arr1, tensor1), axis=None)\n",
        "            arr2 = np.concatenate((arr2, tensor2), axis=None)\n",
        "        del(y_k, s_k)\n",
        "        Bk_sk = B_k.dot(arr2)\n",
        "        term0 = arr1 - Bk_sk\n",
        "        del(Bk_sk)\n",
        "        TERM2 = np.outer( term0, term0 ) / term0.dot(arr2)\n",
        "        del(term0)\n",
        "        B_k  +=  TERM2\n",
        "    return B_k\n",
        "\n",
        "# using separate loss expression for regularization loss\n",
        "def part_loss( regularized_weight_vars, Regularization_Parameters ):\n",
        "\n",
        "    total_loss = tf.constant(0, dtype = tf.float32)\n",
        "    for weight, lamda in zip( regularized_weight_vars, Regularization_Parameters ): \n",
        "        regularization  = lamda * tf.reduce_sum(tf.square(weight))\n",
        "        regularization /= tf.cast(tf.size(weight), dtype=tf.float32)\n",
        "        total_loss     += regularization\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "def remaining_approximation( trainable_weights, Regularization_Parameters ):\n",
        "\n",
        "    # Making the Hyperparameters tf variables (if not)\n",
        "    for idx in range(len(Regularization_Parameters)):\n",
        "        Regularization_Parameters[idx] = tf.Variable( Regularization_Parameters[idx] )\n",
        "    \n",
        "    # Getting indexes of variables being regularized based on dense layer structure. ( NOTE: This only works if only dense layers are regularized )\n",
        "    Weights_Index_ToRegularize = []\n",
        "    for idx in range(len(trainable_weights)):\n",
        "        if len(trainable_weights[idx].shape) == 2:\n",
        "            Weights_Index_ToRegularize.append(idx)\n",
        "\n",
        "    All_Weights = [ tf.Variable(weight) for weight in trainable_weights ]\n",
        "    weight_vars = [ All_Weights[idx] for idx in Weights_Index_ToRegularize]\n",
        "    del(trainable_weights)\n",
        "\n",
        "    with tf.GradientTape(persistent = True) as outer_tape:\n",
        "        with tf.GradientTape() as inner_tape:\n",
        "            loss_ = part_loss( weight_vars, Regularization_Parameters )\n",
        "        grads = inner_tape.gradient(loss_, All_Weights, unconnected_gradients=\"zero\")\n",
        "\n",
        "    second_derivatives = []\n",
        "    for g in grads:\n",
        "        jacob = outer_tape.jacobian( g, Regularization_Parameters, unconnected_gradients ='zero') \n",
        "        second_derivatives.append( tf.convert_to_tensor(jacob) )\n",
        "\n",
        "    # Combining the second-derivative entries\n",
        "    for idx in range(len(second_derivatives)):\n",
        "        tensor = second_derivatives[idx]\n",
        "        if len(tensor.shape) >= 3:\n",
        "            second_derivatives[idx] = tf.reshape(tensor, (tensor.shape[0], -1))\n",
        "    # Concatenate matrix tensors row-wise\n",
        "    second_derivatives = tf.concat(second_derivatives, axis=1)\n",
        "    return second_derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4lQQRm4aWSK"
      },
      "source": [
        "##**Direction Problem**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GBfX9V0kYSF-"
      },
      "outputs": [],
      "source": [
        "# Gurobi optimization\n",
        "def Bilevel_Descent_Direction( GradUObj, Hessian_Follower, delta ): \n",
        "\n",
        "    (Rows_, Columns_)        = Hessian_Follower.shape\n",
        "    Num_regularization_param = len(init_hyperparameters)\n",
        "    \n",
        "    # MODEL AND VARIABLE DECLARATION\n",
        "    m = gp.Model(env = ENV )\n",
        "\n",
        "    Model_Variables = m.addMVar( (Columns_), lb = -1, ub = 1, vtype = 'C' )\n",
        "    # Note: Coefficients are scaled to avoid numerical issues.\n",
        "    m.setObjective( 1e+6 * (GradUObj @ Model_Variables[:-Num_regularization_param]), 1 )\n",
        "    m.addConstr( 1e+5 * (Hessian_Follower @ Model_Variables) <= delta*1e+5 )\n",
        "    m.addConstr( 1e+5 * (Hessian_Follower @ Model_Variables) >= - delta*1e+5 )\n",
        "    # OUTPUT HANDLING\n",
        "    try:\n",
        "        m.optimize()\n",
        "        return m.X, m.ObjVal, m.Runtime\n",
        "    except gp.GurobiError:\n",
        "        m.computeIIS()\n",
        "        m.write(\"IIS_System.ilp\")\n",
        "        return \"Error in LB : GurobiError :: \", m.status\n",
        "\n",
        "# Function to combine the hessian data and giving submatrix with random rows.\n",
        "def random_constraints(percent_rows_used):\n",
        "\n",
        "    hessian_part1     = sr1_hessian_approximation( weight_sets, grad_sets )\n",
        "    remaining_columns = remaining_approximation( trainable_weights_list, init_hyperparameters )\n",
        "    # hessian_full      = tf.concat( [ hessian_part1, tf.transpose(remaining_columns) ], axis = 1 )\n",
        "    hessian_full      = np.concatenate((hessian_part1, remaining_columns.numpy().T),axis=1)\n",
        "    del(hessian_part1, remaining_columns)\n",
        "    num_hyperparams = len(init_hyperparameters)\n",
        "    total_rows     = hessian_full.shape[0]\n",
        "\n",
        "    # ======= PRE-PROCESSING ========\n",
        "    imp_hessian_full = []\n",
        "    for row in hessian_full:\n",
        "        if list(row[-num_hyperparams:])!=[0 for i in range(num_hyperparams)]:\n",
        "            imp_hessian_full.append(row)\n",
        "    del(hessian_full)\n",
        "\n",
        "    imp_hessian_full      = np.array(imp_hessian_full)\n",
        "    Non_zero_rows         =  len(imp_hessian_full)\n",
        "    percent_rows_remained =  Non_zero_rows/total_rows\n",
        "    # print( f\"\\nAfter Pre-processing rows remained :: {percent_rows_remained*100} percent\\n\" )\n",
        "    if percent_rows_remained <= percent_rows_used:\n",
        "        return imp_hessian_full\n",
        "    else:\n",
        "        rows_asked     = percent_rows_used*total_rows\n",
        "        np.random.shuffle(imp_hessian_full)     \n",
        "        # Rows are shuffled. Each row remains unchanged.\n",
        "        imp_hessian_full = imp_hessian_full[ : int(rows_asked), : ]\n",
        "\n",
        "    return imp_hessian_full\n",
        "\n",
        "def unflatten( full_weight_direction ): # Converts flattened directions into weight shapes\n",
        "    result = []\n",
        "    start = 0\n",
        "    for param_size, shape in zip( layer_wise_params, layer_wise_shapes ):\n",
        "\n",
        "        end = start + param_size[0]\n",
        "        flat_list_params = np.array(full_weight_direction[start:end])\n",
        "        start = end\n",
        "\n",
        "        # Converting to tensor object just to use tf.reshape() function.\n",
        "        flat_list_params = tf.convert_to_tensor(flat_list_params)\n",
        "        flat_list_params = tf.reshape(flat_list_params, list(shape))\n",
        "        result.append( flat_list_params )\n",
        "    return result\n",
        "\n",
        "def loss_value( new_weights, new_hyperparams, full_old_weights, Without_GSS = True ): # At every new point, gives loss value by using separate loss object\n",
        "\n",
        "    new_hyperparams = [ float(p.numpy()) for p in new_hyperparams ]\n",
        "    layer_info_ = layer_information( output_shape, hidden_dense_layers, dense_kernel_regularizers=new_hyperparams[:-1], output_kernel_regularizer=new_hyperparams[-1], include_flatten=False )\n",
        "    # model= NeuralNetwork( input_shape, layer_info_).generate_model()\n",
        "    model = CNN( input_shape, output_shape).generate_model( layer_info_)\n",
        "\n",
        "    # Set the new weights as the model's weights. Non-trainable weights fixed as in optuna training.\n",
        "    for idx, wt in zip(index_set,new_weights):\n",
        "        full_old_weights[idx] = wt\n",
        "    model.set_weights(full_old_weights)\n",
        "\n",
        "    # Getting Scores\n",
        "    cce = tf.keras.losses.SparseCategoricalCrossentropy() \n",
        "    y_pred_val = model.predict(x_val)\n",
        "    val_loss_unregularized = cce(y_val, y_pred_val).numpy()\n",
        "\n",
        "    if Without_GSS:\n",
        "        # Remaining LOSS\n",
        "        y_pred_train = model.predict(x_train)\n",
        "        train_loss_unregularized = cce(y_train, y_pred_train).numpy()\n",
        "        y_pred_test = model.predict(x_test)\n",
        "        test_loss_unregularized = cce(y_test, y_pred_test).numpy()\n",
        "        # Accuracy\n",
        "        sca = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "        train_acc = sca(y_train, y_pred_train).numpy()\n",
        "        val_acc = sca(y_val, y_pred_val).numpy()\n",
        "        test_acc = sca(y_test, y_pred_test).numpy()\n",
        "        accuracy = [ train_acc, val_acc, test_acc ]\n",
        "        loss     = [ train_loss_unregularized, val_loss_unregularized, test_loss_unregularized ]\n",
        "        return loss, accuracy\n",
        "    else:\n",
        "        return val_loss_unregularized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlBwIDtt6JsO"
      },
      "source": [
        "##**Running Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bTHI49mUaAJ8"
      },
      "outputs": [],
      "source": [
        "# GETTING OBJECTIVE COEFFICIENTS\n",
        "time1 = datetime.now()\n",
        "\n",
        "Validation_Coefficients = gradient_validation( batch_size = 128 )\n",
        "Validation_Coefficients = tf.concat( [tf.reshape(tensor, [-1]) for tensor in Validation_Coefficients], axis=0 ).numpy()\n",
        "\n",
        "time2 = datetime.now()\n",
        "coef_time = (time2 - time1).total_seconds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlttSM4RlRc4",
        "outputId": "ae45771e-d9a3-4d7e-feaa-2df2df8730ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x7f911035df30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x7f91ade988b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 10ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 11ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 11ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "19/19 [==============================] - 1s 27ms/step\n",
            "313/313 [==============================] - 3s 10ms/step\n",
            "Trainable parameters: 32,986\n",
            "13/13 [==============================] - 1s 12ms/step\n",
            "19/19 [==============================] - 0s 10ms/step\n",
            "313/313 [==============================] - 3s 10ms/step\n",
            "\n",
            "Optimal LOSS->  [0.79676545, 1.2854482, 1.324376] \n",
            "Accuracy ->  [0.76, 0.686, 0.5660909] \n",
            "t_star->  0.1668637919568067\n",
            "\n",
            " Total time taken for final improvement :: 77.168993 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "      T_star  Training_Loss  Validation_Loss  Testing_Loss  Training_Accuracy  \\\n",
            "0  0.000000       0.889145         1.442169      1.439843           0.711667   \n",
            "1  0.166864       0.796765         1.285448      1.324376           0.760000   \n",
            "\n",
            "   Validation_Accuracy  Testing_Accuracy     Runtime  \n",
            "0                0.641          0.543727   64.893777  \n",
            "1                0.686          0.566091  135.787038  \n"
          ]
        }
      ],
      "source": [
        "percentage_of_submatrix = [ 0.01 ]\n",
        "num_hyperparams         = len(init_hyperparameters)\n",
        "\n",
        "trials = 1\n",
        "for trial in range(trials):\n",
        "    for rows_ in percentage_of_submatrix:\n",
        "        # GETTING OBJECTIVE COEFFICIENTS AND CONSTRAINT MATRIX\n",
        "        time1 = datetime.now()\n",
        "        constraint_matrix = random_constraints(rows_) # Set percentage of hessian to be used in the direction problem\n",
        "        time2 = datetime.now()\n",
        "        data_collection_time = (time2 - time1).total_seconds() + coef_time\n",
        "\n",
        "        # GETTING DIRECTIONS FROM LINEAR PROGRAM\n",
        "        Directions_ = Bilevel_Descent_Direction( Validation_Coefficients, constraint_matrix, 1e-4)\n",
        "        linear_problem_runtime = Directions_[-1]\n",
        "\n",
        "        Directions_ = np.array(Directions_[0])\n",
        "        Directions_ = Directions_/np.linalg.norm(Directions_)\n",
        "\n",
        "        time1 = datetime.now()\n",
        "\n",
        "        layer_wise_shapes = [ val.shape for val in weight_sets[0] ]  # Only trainable layers\n",
        "        layer_wise_params = [ val.flatten().shape for val in weight_sets[0] ]\n",
        "\n",
        "        Weight_directions     = unflatten( Directions_[:-num_hyperparams] )\n",
        "        Hyperparam_directions = Directions_[-num_hyperparams:]\n",
        "\n",
        "        validation_loss_ = 1e10\n",
        "\n",
        "        def minimize_function(t,Without_GSS=False):\n",
        "            hyperparams_t   = [ tf.math.add( i,t*j ) for i,j in zip( init_hyperparameters, Hyperparam_directions  ) ]\n",
        "            layer_weights_t = []\n",
        "            for i,j in zip( trainable_weights_list, Weight_directions ):\n",
        "                new_weight = tf.math.add( tf.convert_to_tensor(i.numpy(), dtype = tf.float64),t*j )\n",
        "                layer_weights_t.append( new_weight )\n",
        "            if Without_GSS:\n",
        "                loss, acc = loss_value( layer_weights_t, hyperparams_t, full_weights_list, Without_GSS )\n",
        "                return loss, acc\n",
        "            else:\n",
        "                loss = loss_value( layer_weights_t, hyperparams_t, full_weights_list, Without_GSS )\n",
        "                return loss\n",
        "\n",
        "        def interval_search( init_t = 0, step = 0.1 ):\n",
        "            a = init_t\n",
        "            b = init_t + step\n",
        "\n",
        "            loss_a, loss_b = minimize_function(a), minimize_function(b)\n",
        "            i=0\n",
        "            while loss_b < loss_a:\n",
        "                i+=1\n",
        "                a,loss_a = b, loss_b\n",
        "                b = init_t + (2**i) * step\n",
        "                loss_b = minimize_function(b)\n",
        "            if i==0:\n",
        "                return a,b\n",
        "            elif i==1:\n",
        "                return init_t,b\n",
        "            else:\n",
        "                return init_t + (2**(i-2)) * step, b\n",
        "\n",
        "        def gss(tol=1e-5):\n",
        "            a,b = interval_search()\n",
        "            invphi = (math.sqrt(5) - 1) / 2  # 1 / phi\n",
        "            invphi2 = (3 - math.sqrt(5)) / 2  # 1 / phi^2\n",
        "\n",
        "            (a, b) = (min(a, b), max(a, b))\n",
        "            h = b - a\n",
        "            if h <= tol:\n",
        "                return (a, b)\n",
        "\n",
        "            # Required steps to achieve tolerance\n",
        "            n = int(math.ceil(math.log(tol / h) / math.log(invphi)))\n",
        "\n",
        "            c = a + invphi2 * h\n",
        "            d = a + invphi * h\n",
        "            yc = minimize_function(c)\n",
        "            yd = minimize_function(d)\n",
        "\n",
        "            for k in range(n - 1):\n",
        "                if yc < yd:  # yc > yd to find the maximum\n",
        "                    b = d\n",
        "                    d = c\n",
        "                    yd = yc\n",
        "                    h = invphi * h\n",
        "                    c = a + invphi2 * h\n",
        "                    yc = minimize_function(c)\n",
        "                else:\n",
        "                    a = c\n",
        "                    c = d\n",
        "                    yc = yd\n",
        "                    h = invphi * h\n",
        "                    d = a + invphi * h\n",
        "                    yd = minimize_function(d)\n",
        "\n",
        "            if yc < yd:\n",
        "                return (a, d)\n",
        "            else:\n",
        "                return (c, b)\n",
        "\n",
        "\n",
        "        interval_ = gss(tol=1e-4 ) \n",
        "        time2 = datetime.now()\n",
        "\n",
        "        optimal_t = (interval_[0] + interval_[1])/2\n",
        "        loss, accuracy = minimize_function( optimal_t, Without_GSS=True) \n",
        "        init_loss, init_accuracy = minimize_function( 0, Without_GSS=True)\n",
        "        print( \"\\nOptimal LOSS-> \", loss, \"\\nAccuracy -> \", accuracy, \"\\nt_star-> \", optimal_t )\n",
        "        solution_search_runtime = (time2-time1).total_seconds()\n",
        "        print(\"\\n Total time taken for final improvement ::\", solution_search_runtime, \"\\n\\n\")\n",
        "\n",
        "        # ==================== Saving Results ================================\n",
        "        DF = { \"T_star\"            : [ 0, optimal_t ],\n",
        "              \"Training_Loss\"      : [ init_loss[0], loss[0] ],\n",
        "              \"Validation_Loss\"    : [ init_loss[1], loss[1] ],\n",
        "              \"Testing_Loss\"       : [ init_loss[2], loss[2] ],\n",
        "              \"Training_Accuracy\"  : [ init_accuracy[0], accuracy[0] ],\n",
        "              \"Validation_Accuracy\": [ init_accuracy[1], accuracy[1] ],\n",
        "              \"Testing_Accuracy\"   : [ init_accuracy[2], accuracy[2] ],\n",
        "              \"Runtime\"            :[ optuna_time.total_seconds(), data_collection_time + linear_problem_runtime + solution_search_runtime ]}\n",
        "        \n",
        "        DF = pd.DataFrame.from_dict(DF)\n",
        "        DF.to_excel(full_output_directory)\n",
        "\n",
        "        print(\"\\n\\n\",DF)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d_LdJJTgGYLI"
      },
      "execution_count": 13,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_uDuLQvSsNkH",
        "IOtTTaR7BAt-",
        "-4lQQRm4aWSK"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}