{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpSFh2BjjTYq"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "\n",
        "import tensorflow as tf\n",
        "import optuna\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "import math\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GOLBAL VARIABLES NEEDED TO RUN THE EXPERIMENT\n",
        "\n",
        "# 1] ENTER DATA DIRECTORIES\n",
        "base_dir  = \"/content/drive/MyDrive/CIFAR_Dataset/CIFAR_10/Batch3_1+10K\"\n",
        "train_dir = base_dir+'/cifar10_train.npz'\n",
        "val_dir   = base_dir+'/cifar10_val.npz'\n",
        "test_dir  = base_dir+'/cifar10_test.npz'\n",
        "# 2] ENTER DIRECTORY TO SAVE OPTUNA's TRAINED MODEL\n",
        "OPTUNA_MODEL_DIRECTORY = \"/content/drive/MyDrive/CIFAR_Dataset/CIFAR_10/resnet_optuna.pickle\"\n",
        "\n",
        "TYPE_OF_SAMPLER = 'grid' #  'grid', 'random', 'qmc', 'tpe' \n",
        "NUM_OPTUNA_TRIALS = 10"
      ],
      "metadata": {
        "id": "BVb9XYleuXxe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**OPTUNA STUDY**"
      ],
      "metadata": {
        "id": "75GpXu5Zyv9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Loading Data**"
      ],
      "metadata": {
        "id": "p3xYgWyVuZVt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qOpQqPRDl1Qh"
      },
      "outputs": [],
      "source": [
        "# Enter training, validation and testing dataset directories for CIFAR-10 datasets.\n",
        "def preprocess_image_input(input_images): # Only used when resent50 is selected as model_type below\n",
        "  input_images = input_images.astype('float32')\n",
        "  output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
        "  return output_ims\n",
        "\n",
        "def data_loader( train_dir, val_dir, test_dir, model_type ): # model_type : 'cnn' OR 'resnet50' ( Please specify only one of these in sttring format )\n",
        "\n",
        "    train_dataset = np.load(train_dir)\n",
        "    val_dataset   = np.load(val_dir)\n",
        "    test_dataset  = np.load(test_dir)\n",
        "\n",
        "    y_train = train_dataset['y_train'].astype(\"float32\")\n",
        "    y_val   = val_dataset['y_val'].astype(\"float32\") \n",
        "    y_test  = test_dataset['y_test'].astype(\"float32\") \n",
        "\n",
        "    x_train = train_dataset['x_train'].astype(\"float32\")\n",
        "    x_val   = val_dataset['x_val'].astype(\"float32\")\n",
        "    x_test  = test_dataset['x_test'].astype(\"float32\") \n",
        "\n",
        "    if model_type == 'cnn':\n",
        "        x_train, x_val, x_test = x_train/255, x_val/255, x_test/255\n",
        "    elif model_type == 'resnet50':\n",
        "        x_train = preprocess_image_input(x_train)\n",
        "        x_val = preprocess_image_input(x_val)\n",
        "        x_test = preprocess_image_input(x_test)\n",
        "    elif model_type not in ['cnn', 'resnet50']:\n",
        "        raise ValueError('Error: Please enter correct \\'model_type\\' variable value. Correct values are \\'cnn\\' or \\'resnet50\\' (strings).')\n",
        "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
        "\n",
        "## ============================================  LOADING DATA  =====================================================\n",
        "x_train, x_val, x_test, y_train, y_val, y_test = data_loader( train_dir, val_dir, test_dir, model_type='resnet50' )\n",
        "\n",
        "input_shape  = x_train.shape[1:]\n",
        "output_shape = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvjtltoIrR_J"
      },
      "source": [
        "##**Custom Model Defining**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wE3rshsvKcuO"
      },
      "outputs": [],
      "source": [
        "class CNN:\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "    def generate_model(self, layer_info = None ):\n",
        "        UpSampling = 224/self.input_shape[0]\n",
        "\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Input(shape=self.input_shape))\n",
        "        model.add( tf.keras.layers.UpSampling2D(size=(UpSampling,UpSampling)))\n",
        "        model.add(tf.keras.applications.resnet50.ResNet50(include_top = False, weights = 'imagenet', input_shape = (224, 224, 3),  pooling = 'avg'))\n",
        "        if layer_info is not None:\n",
        "            for layer in layer_info[:-1]:\n",
        "                layer_params = layer['params']\n",
        "                if layer['type'] == 'dense':\n",
        "                    model.add(tf.keras.layers.Dense(**layer_params))\n",
        "                    model.add(tf.keras.layers.BatchNormalization())\n",
        "            for layer in layer_info[-1:]:\n",
        "                layer_params = layer['params']\n",
        "                if layer['type'] == 'dense':\n",
        "                    model.add(tf.keras.layers.Dense(**layer_params))\n",
        "        model.layers[1].trainable = False\n",
        "        # Calculate the number of trainable parameters in the model\n",
        "        trainable_count = sum(tf.keras.backend.count_params(weights) for weights in model.trainable_weights)\n",
        "        print(f\"Trainable parameters: {trainable_count:,}\")\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R3-RzK33q5sy"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def loss_function_optuna( y_dataset, logits, loss ): # logits = model(x_dataset)\n",
        "    total_loss = loss(y_dataset, logits)\n",
        "    total_loss = tf.cast( total_loss, dtype=tf.float32 )\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wAlCfWNqrZC9"
      },
      "outputs": [],
      "source": [
        "def fmin_loss( model, loss_function, optimizer, batch_size , epochs, record = True ):      # lamda, not exp(lamda), Works with both tf.Variable and tf.constant type lambda input, (or just scalar)\n",
        "    # total_loss0 = 1e20\n",
        "    train_df = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
        "    train_df = train_df.shuffle(buffer_size = 1024).batch(batch_size)\n",
        "\n",
        "    All_Epoch_Gradients, All_Epoch_Weights = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        weights0 = [var.numpy() for var in model.trainable_weights] # Getting only trainable weights at which the gradient is being calculated.\n",
        "        # Note : model.get_weights() retrieves all the weights (including non-trainable)\n",
        "\n",
        "        Step_Gradient, Num_batch = [], 0\n",
        "        \n",
        "        for step,(x_train_,y_train_) in enumerate(train_df):\n",
        "            # print(\"Step == \", step)\n",
        "            with tf.GradientTape(persistent = True) as tape:\n",
        "\n",
        "                logits = model(x_train_, training=True)\n",
        "                total_loss1 = loss_function_optuna( y_train_, logits, loss = loss_function ) \n",
        "\n",
        "            vars_list = model.trainable_weights\n",
        "            grads = tape.gradient(total_loss1, vars_list)      # for ref  - https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough \n",
        "            optimizer.apply_gradients(zip(grads,vars_list))\n",
        "\n",
        "            if record : \n",
        "                if step == 0 : \n",
        "                    Step_Gradient = grads\n",
        "                    # print( Step_Gradient )\n",
        "                else:\n",
        "                    for idx in range(len(Step_Gradient)):\n",
        "                        Step_Gradient[idx] =  tf.add(Step_Gradient[idx], grads[idx])\n",
        "            Num_batch = step\n",
        "        \n",
        "    # total_loss0   = total_loss1\n",
        "        if record : \n",
        "            Step_Gradient = [ i/Num_batch for i in Step_Gradient ] \n",
        "            All_Epoch_Gradients.append( Step_Gradient )\n",
        "            All_Epoch_Weights.append(weights0)\n",
        "            \n",
        "    if record :\n",
        "        return All_Epoch_Gradients, All_Epoch_Weights\n",
        "    else: \n",
        "        return 0, 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riEC0fSDGxRP"
      },
      "source": [
        "####Note: Training and Local Tuning is done based on Loss and not the Accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3TADO0MDrjwS"
      },
      "outputs": [],
      "source": [
        "NUMBER_OF_DENSE_LAYERS = 2\n",
        "\n",
        "def optuna_optimizer(trial):\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    alphas = [ trial.suggest_float(f'regularization{i}', 1e-6, 1e-1, log=True) for i in range(NUMBER_OF_DENSE_LAYERS) ]\n",
        "\n",
        "    layer_info_ = [ {'type': 'dense', 'params': {'units': 16, 'activation': 'relu', 'kernel_regularizer':tf.keras.regularizers.l2(i)}} for i in alphas[:-1]]\n",
        "    layer_info_ += [ {'type': 'dense', 'params': {'units': output_shape, 'activation': 'softmax', 'kernel_regularizer':tf.keras.regularizers.l2(alphas[-1])}} ]\n",
        "\n",
        "    model = CNN( input_shape, output_shape).generate_model( layer_info_ )\n",
        "    \n",
        "    # Optimizing\n",
        "    optimizer         = tf.keras.optimizers.Adam(0.01)\n",
        "    loss_function     = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    All_Epoch_Gradients, All_Epoch_Weights = fmin_loss( model, loss_function, optimizer,  128,  10, True)\n",
        "\n",
        "    # Getting Scores\n",
        "    cce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    sca = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    y_pred_train = model.predict(x_train)\n",
        "    train_loss_unregularized = cce(y_train, y_pred_train).numpy()\n",
        "    train_acc = sca(y_train, y_pred_train).numpy()\n",
        "\n",
        "    y_pred_val = model.predict(x_val)\n",
        "    val_loss_unregularized = cce(y_val, y_pred_val).numpy()\n",
        "    val_acc = sca(y_val, y_pred_val).numpy()\n",
        "\n",
        "    y_pred_test = model.predict(x_test)\n",
        "    test_loss_unregularized = cce(y_test, y_pred_test).numpy()\n",
        "    test_acc = sca(y_test, y_pred_test).numpy()\n",
        "\n",
        "    print(\"\\nTraining:  Loss \", train_loss_unregularized, \"  Accuracy\", train_acc*100 )\n",
        "    print(\"Validation: Loss \", val_loss_unregularized, \"  Accuracy\", val_acc*100 )\n",
        "    print(\"Test:       Loss\", test_loss_unregularized, \"  Accuracy\", test_acc*100, \"\\n\\n\")\n",
        "\n",
        "    with open(\"{}.pickle\".format(trial.number), \"wb\") as fout:\n",
        "        pkl.dump(model, fout)\n",
        "\n",
        "    with open(\"training_info_{}.pickle\".format(trial.number), \"wb\") as fout:\n",
        "        Dict_ = { \"Gradients\" : All_Epoch_Gradients, \"Weights\" : All_Epoch_Weights }\n",
        "        pkl.dump(Dict_, fout)\n",
        "    \n",
        "    score = val_loss_unregularized\n",
        "    return(score)\n",
        "\n",
        "\n",
        "def optuna_training( num_trials ):\n",
        "\n",
        "    time1 = datetime.now()\n",
        "\n",
        "    if TYPE_OF_SAMPLER == 'grid': \n",
        "        search_space = { f\"regularization{i}\" : list(np.linspace(1e-6,1e-1,10)) for i in range(NUMBER_OF_DENSE_LAYERS) }\n",
        "        study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space), direction = \"minimize\")\n",
        "    elif TYPE_OF_SAMPLER == 'random': \n",
        "        study = optuna.create_study(sampler=optuna.samplers.RandomSampler(), direction = \"minimize\")\n",
        "    elif TYPE_OF_SAMPLER == 'qmc': \n",
        "        study = optuna.create_study(sampler=optuna.samplers.QMCSampler(), direction = \"minimize\")\n",
        "    elif TYPE_OF_SAMPLER == 'tpe': \n",
        "        study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction = \"minimize\")\n",
        "    else:\n",
        "        print(\"...Mention the correct sampler name...\")\n",
        "\n",
        "    study.optimize(optuna_optimizer, n_trials = num_trials)\n",
        "\n",
        "    print('\\n\\n')\n",
        "    trial = study.best_trial\n",
        "    print(\"Best Score: \", trial.value)\n",
        "    print(\"Best Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"  {}: {}\".format(key, value))\n",
        "\n",
        "    print( \"\\n\\n \", \"Trial Number: \", trial.number, \"\\n\" )\n",
        "\n",
        "    time2 = datetime.now()\n",
        "    delta = time2 - time1\n",
        "    print(f\"Time difference is {delta.total_seconds()} seconds\")\n",
        "\n",
        "    # Loading Best OPTUNA model to get initial feasible weights for trainable layers\n",
        "    with open(\"{}.pickle\".format(trial.number), \"rb\") as fin:\n",
        "        best_clf = pkl.load(fin)\n",
        "\n",
        "    with open(\"training_info_{}.pickle\".format(trial.number), \"rb\") as fin_:\n",
        "        wt_grad = pkl.load(fin_)\n",
        "\n",
        "    # Getting Optimal Model's Weights and Gradients values for Hessian Approximations\n",
        "    weight_sets = wt_grad['Weights']\n",
        "    grad_sets   = wt_grad['Gradients']\n",
        "\n",
        "    # Getting Optimal Model's Weights and Hyperparameters\n",
        "    full_weights_list          = best_clf.get_weights()\n",
        "    trainable_weights_list     = best_clf.trainable_weights\n",
        "    init_hyperparameters = [ tf.Variable(value) for key, value in trial.params.items() ]\n",
        "\n",
        "    return trainable_weights_list, full_weights_list, init_hyperparameters, weight_sets, grad_sets, delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfHTK01RyTWH",
        "outputId": "af6b1d47-c7fe-4142-857f-2aeb485e2ede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-23 13:59:43,493]\u001b[0m A new study created in memory with name: no-name-fba4b8da-4f2b-4fe4-b73c-4cbb98bfeadc\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 5s 0us/step\n",
            "Trainable parameters: 32,986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fa5a0286e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fa5a0286e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 2s 27ms/step\n",
            "13/13 [==============================] - 0s 30ms/step\n",
            "313/313 [==============================] - 3s 11ms/step\n",
            "\n",
            "Training:  Loss  0.8891305   Accuracy 71.16666436195374\n",
            "Validation: Loss  1.4421617   Accuracy 64.0999972820282\n",
            "Test:       Loss 1.4398409   Accuracy 54.37272787094116 \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-23 14:00:28,815]\u001b[0m Trial 0 finished with value: 1.4421616792678833 and parameters: {'regularization0': 0.1, 'regularization1': 0.011112}. Best is trial 0 with value: 1.4421616792678833.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 32,986\n",
            "19/19 [==============================] - 1s 10ms/step\n",
            "13/13 [==============================] - 0s 10ms/step\n",
            "313/313 [==============================] - 3s 10ms/step\n",
            "\n",
            "Training:  Loss  0.9514213   Accuracy 71.66666388511658\n",
            "Validation: Loss  1.4544638   Accuracy 63.70000243186951\n",
            "Test:       Loss 1.4903299   Accuracy 53.93636226654053 \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-05-23 14:00:48,381]\u001b[0m Trial 1 finished with value: 1.4544638395309448 and parameters: {'regularization0': 0.05555600000000001, 'regularization1': 0.1}. Best is trial 0 with value: 1.4421616792678833.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Best Score:  1.4421616792678833\n",
            "Best Params: \n",
            "  regularization0: 0.1\n",
            "  regularization1: 0.011112\n",
            "\n",
            "\n",
            "  Trial Number:  0 \n",
            "\n",
            "Time difference is 64.893777 seconds\n"
          ]
        }
      ],
      "source": [
        "trainable_weights_list, full_weights_list, init_hyperparameters, weight_sets, grad_sets, optuna_time = optuna_training(NUM_OPTUNA_TRIALS)\n",
        "\n",
        "model__ = [ trainable_weights_list, full_weights_list, init_hyperparameters, weight_sets, grad_sets, optuna_time ]\n",
        "# LOADING OPTUNA TRAINED MODELS\n",
        "with open(OPTUNA_MODEL_DIRECTORY, \"wb\") as fout:\n",
        "        pkl.dump(model__, fout)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vh9VgN-Ay_Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o79OJUrc0m13"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "CQEJOPWprMm9",
        "HvjtltoIrR_J",
        "IOtTTaR7BAt-",
        "-4lQQRm4aWSK",
        "RlBwIDtt6JsO"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}